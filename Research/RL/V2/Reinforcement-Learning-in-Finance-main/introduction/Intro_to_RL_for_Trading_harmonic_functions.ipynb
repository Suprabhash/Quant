{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Intro to RL for Trading: harmonic functions.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "HjUmgGL28VkP"
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PgYTJF7FOeyI"
   },
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhO6h_Uc__Wi"
   },
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sN_kNTX-AZ2A"
   },
   "source": [
    "class Environment:\n",
    "    '''\n",
    "        Simple environment\n",
    "    '''\n",
    "    def __init__(self, length = 100, normalize = True, noise = True, data = []):\n",
    "        self.length = length\n",
    "\n",
    "        if len(data) == 0:\n",
    "            # self.data = -pd.DataFrame(np.arange(self.length))\n",
    "            self.data = pd.DataFrame(np.sin(np.arange(length)/30.0))\n",
    "        else:\n",
    "            self.data = data\n",
    "\n",
    "        if noise:\n",
    "            self.data += pd.DataFrame(np.random.normal(0, 0.1, size=(length, )))\n",
    "\n",
    "        if normalize:\n",
    "            self.data = (self.data - self.data.min()) / (self.data.max() - self.data.min())\n",
    "\n",
    "    def get_state(self, time, lookback, diff = True):\n",
    "        window = self.data.iloc[time-lookback:time]\n",
    "        if diff: window = window.diff().fillna(0.0)\n",
    "        return window\n",
    "\n",
    "    def get_reward(self, action, action_time, reward_time, coef = 100):\n",
    "        # 0 => long; 1 => hold, 2 => short\n",
    "        if action == 0:\n",
    "            action = 1\n",
    "        elif action == 1:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = -1\n",
    "        price_now = self.data.iloc[action_time]\n",
    "        price_reward = self.data.iloc[reward_time]\n",
    "        price_diff = (price_reward - price_now) / price_now\n",
    "        reward = np.sign(price_diff) * action * coef\n",
    "        return reward.values.tolist()[0]"
   ],
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rJQnbrkIhISN"
   },
   "source": [
    "import collections"
   ],
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bzCQw17UAmRH"
   },
   "source": [
    "class BuyHoldSellAgent:\n",
    "    '''\n",
    "        A simple agent\n",
    "    '''\n",
    "    def __init__(self, state_shape = 10, action_shape = 3, experience_size = 100):\n",
    "        self.state_shape = state_shape\n",
    "        self.action_shape = action_shape\n",
    "        self.experience_size = experience_size\n",
    "        self.model = self.init_model()\n",
    "        self.experience = collections.deque(maxlen=self.experience_size)\n",
    "\n",
    "    def init_model(self):\n",
    "        inputs = tfk.Input(shape=(self.state_shape,))\n",
    "        x = tfk.layers.Dense(10, activation='relu')(inputs)\n",
    "        outputs = tfk.layers.Dense(self.action_shape, activation='linear')(x)\n",
    "        model = tfk.Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer=tfk.optimizers.Adam(0.1), loss='mse', metrics='mse')\n",
    "        return model\n",
    "\n",
    "    def save_experience(self, state_i, q_value_i, action_i, reward_i, state_i_1):\n",
    "        self.experience.append({\n",
    "            'state_i': state_i,\n",
    "            'q_value_i': q_value_i,\n",
    "            'action_i': action_i,\n",
    "            'reward_i': reward_i,\n",
    "            'state_i_1': state_i_1\n",
    "        })\n",
    "\n",
    "    def replay_experience(self, alpha, gamma, sample_size):\n",
    "        X, Y = [], []\n",
    "        indices_sampled = np.random.choice(len(self.experience), sample_size, replace=False)\n",
    "        for i, e in enumerate(self.experience):\n",
    "            if i in indices_sampled:\n",
    "                state_i, action_i, reward_i, q_value_i = e['state_i'], e['action_i'], e['reward_i'], e['q_value_i']\n",
    "                state_i_1 = e['state_i_1']\n",
    "                q_value_i_1 = self.model.predict(np.expand_dims(state_i_1, 0))[0]\n",
    "                y_i = np.zeros(self.action_shape)\n",
    "                y_i[:] = q_value_i[:]\n",
    "                y_i[action_i] = (1 - alpha) * y_i[action_i] + alpha * (reward_i + gamma * max(q_value_i_1))\n",
    "                X.append(state_i)\n",
    "                Y.append(y_i)\n",
    "        X, Y = np.array(X), np.array(Y)\n",
    "        self.model.fit(X, Y, epochs=1, batch_size=sample_size, verbose=0)\n",
    "\n",
    "    def get_value_action_value(self, state):\n",
    "        pred = self.model.predict(np.expand_dims(state, 0))\n",
    "        return pred.flatten()"
   ],
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OMdkUloYZ1B"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kru4HB2XYaeg"
   },
   "source": [
    "epochs = 5\n",
    "gamma = 0.9\n",
    "epsilon = 0.95\n",
    "alpha = 0.9"
   ],
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nd9X3rnAami_"
   },
   "source": [
    "DATASET_LENGTH = 250\n",
    "WINDOW_SHAPE = 5\n",
    "REWARD_TIME = 1\n",
    "ACTIONS_SHAPE = 3\n",
    "SAMPLE_SIZE = 16"
   ],
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "environment = Environment(DATASET_LENGTH, True, False)\n",
    "agent = BuyHoldSellAgent(WINDOW_SHAPE, ACTIONS_SHAPE)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31uD9MugAopP",
    "outputId": "2e36e1ae-f66e-474b-92d6-b7162add566b"
   },
   "source": [
    "for i in range(epochs):\n",
    "\n",
    "    learning_progress = []\n",
    "    for j in range(WINDOW_SHAPE, DATASET_LENGTH - REWARD_TIME, REWARD_TIME): \n",
    "\n",
    "        # 1. getting current state\n",
    "        state_j = environment.get_state(j, WINDOW_SHAPE)\n",
    "\n",
    "        print(\"*\"*100)\n",
    "        # print(f\"State_j: {state_j}\")\n",
    "\n",
    "\n",
    "        q_value_j = agent.get_value_action_value(state_j)\n",
    "\n",
    "\n",
    "        # print(f\"Q_value_j: {q_value_j}\")\n",
    "\n",
    "        # 2. acting in this state\n",
    "        if (np.random.random() < epsilon):\n",
    "            action = np.random.randint(0, ACTIONS_SHAPE)\n",
    "        else:\n",
    "            action = (np.argmax(q_value_j))    \n",
    "\n",
    "\n",
    "\n",
    "        # 3. getting reward from this action\n",
    "        reward_value_j = environment.get_reward(action, j, j+REWARD_TIME)\n",
    "        learning_progress.append(reward_value_j)\n",
    "\n",
    "        # 4. getting next state and value there\n",
    "        state_j_1 = environment.get_state(j+1, WINDOW_SHAPE)\n",
    "\n",
    "        # 5. save this experience\n",
    "        agent.save_experience(state_j, q_value_j, action, reward_value_j, state_j_1)\n",
    "\n",
    "        if j > SAMPLE_SIZE * 2:\n",
    "            # 6. train on samples from experience\n",
    "            agent.replay_experience(alpha, gamma, SAMPLE_SIZE)\n",
    "\n",
    "    if epsilon > 0.1:\n",
    "        epsilon -= (1.0/epochs)\n",
    "\n",
    "    print('Epoch', i, '...', np.mean(learning_progress))\n",
    "    learning_progress = []"
   ],
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "0   -100.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0    100.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0    100.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0    100.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0   -100.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0    100.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0   -100.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0   -100.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0    0.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0    100.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0   -100.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0    100.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0   -100.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0    0.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0   -100.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0    0.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0    100.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0    100.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0    0.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0    0.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0    100.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0    100.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0    0.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0    100.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0    0.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0    100.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0   -100.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0   -100.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0    100.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0    100.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0    0.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0    100.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0    0.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0    100.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0    0.0\n",
      "dtype: float64\n",
      "****************************************************************************************************\n",
      "0    100.0\n",
      "dtype: float64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [33]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     34\u001B[0m     agent\u001B[38;5;241m.\u001B[39msave_experience(state_j, q_value_j, action, reward_value_j, state_j_1)\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m j \u001B[38;5;241m>\u001B[39m SAMPLE_SIZE \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[0;32m     37\u001B[0m         \u001B[38;5;66;03m# 6. train on samples from experience\u001B[39;00m\n\u001B[1;32m---> 38\u001B[0m         \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreplay_experience\u001B[49m\u001B[43m(\u001B[49m\u001B[43malpha\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgamma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mSAMPLE_SIZE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m epsilon \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0.1\u001B[39m:\n\u001B[0;32m     41\u001B[0m     epsilon \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m1.0\u001B[39m\u001B[38;5;241m/\u001B[39mepochs)\n",
      "Input \u001B[1;32mIn [29]\u001B[0m, in \u001B[0;36mBuyHoldSellAgent.replay_experience\u001B[1;34m(self, alpha, gamma, sample_size)\u001B[0m\n\u001B[0;32m     34\u001B[0m state_i, action_i, reward_i, q_value_i \u001B[38;5;241m=\u001B[39m e[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstate_i\u001B[39m\u001B[38;5;124m'\u001B[39m], e[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maction_i\u001B[39m\u001B[38;5;124m'\u001B[39m], e[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreward_i\u001B[39m\u001B[38;5;124m'\u001B[39m], e[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mq_value_i\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     35\u001B[0m state_i_1 \u001B[38;5;241m=\u001B[39m e[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstate_i_1\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m---> 36\u001B[0m q_value_i_1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexpand_dims\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate_i_1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     37\u001B[0m y_i \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_shape)\n\u001B[0;32m     38\u001B[0m y_i[:] \u001B[38;5;241m=\u001B[39m q_value_i[:]\n",
      "File \u001B[1;32mc:\\users\\suprabhashsahu\\desktop\\strategyresearch\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     62\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 64\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[0;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32mc:\\users\\suprabhashsahu\\desktop\\strategyresearch\\venv\\lib\\site-packages\\keras\\engine\\training.py:1951\u001B[0m, in \u001B[0;36mModel.predict\u001B[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1944\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[0;32m   1945\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m   1946\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUsing Model.predict with MultiWorkerMirroredStrategy or \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m   1947\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTPUStrategy and AutoShardPolicy.FILE might lead to out-of-order \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m   1948\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresult. Consider setting it to AutoShardPolicy.DATA.\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m   1949\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m-> 1951\u001B[0m data_handler \u001B[38;5;241m=\u001B[39m \u001B[43mdata_adapter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_data_handler\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1952\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1953\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1954\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteps_per_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1955\u001B[0m \u001B[43m    \u001B[49m\u001B[43minitial_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1956\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1957\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_queue_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_queue_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1958\u001B[0m \u001B[43m    \u001B[49m\u001B[43mworkers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mworkers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1959\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_multiprocessing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_multiprocessing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1960\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1961\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteps_per_execution\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_steps_per_execution\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1963\u001B[0m \u001B[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001B[39;00m\n\u001B[0;32m   1964\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(callbacks, callbacks_module\u001B[38;5;241m.\u001B[39mCallbackList):\n",
      "File \u001B[1;32mc:\\users\\suprabhashsahu\\desktop\\strategyresearch\\venv\\lib\\site-packages\\keras\\engine\\data_adapter.py:1399\u001B[0m, in \u001B[0;36mget_data_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   1397\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cluster_coordinator\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m   1398\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m _ClusterCoordinatorDataHandler(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m-> 1399\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataHandler(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\users\\suprabhashsahu\\desktop\\strategyresearch\\venv\\lib\\site-packages\\keras\\engine\\data_adapter.py:1149\u001B[0m, in \u001B[0;36mDataHandler.__init__\u001B[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001B[0m\n\u001B[0;32m   1146\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_steps_per_execution \u001B[38;5;241m=\u001B[39m steps_per_execution\n\u001B[0;32m   1148\u001B[0m adapter_cls \u001B[38;5;241m=\u001B[39m select_data_adapter(x, y)\n\u001B[1;32m-> 1149\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_adapter \u001B[38;5;241m=\u001B[39m \u001B[43madapter_cls\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1150\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1151\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1152\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1153\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msteps_per_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1154\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43minitial_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1155\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1156\u001B[0m \u001B[43m    \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshuffle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1157\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_queue_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_queue_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1158\u001B[0m \u001B[43m    \u001B[49m\u001B[43mworkers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mworkers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1159\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_multiprocessing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_multiprocessing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1160\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdistribution_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistribute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_strategy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1161\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1163\u001B[0m strategy \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mdistribute\u001B[38;5;241m.\u001B[39mget_strategy()\n\u001B[0;32m   1165\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_current_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[1;32mc:\\users\\suprabhashsahu\\desktop\\strategyresearch\\venv\\lib\\site-packages\\keras\\engine\\data_adapter.py:275\u001B[0m, in \u001B[0;36mTensorLikeDataAdapter.__init__\u001B[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001B[0m\n\u001B[0;32m    263\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shuffle \u001B[38;5;241m=\u001B[39m shuffle\n\u001B[0;32m    264\u001B[0m \u001B[38;5;66;03m# Vectorized version of shuffle.\u001B[39;00m\n\u001B[0;32m    265\u001B[0m \u001B[38;5;66;03m# This is a performance improvement over using `from_tensor_slices`.\u001B[39;00m\n\u001B[0;32m    266\u001B[0m \u001B[38;5;66;03m# The indices of the data are shuffled and batched, and these indices\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    272\u001B[0m \u001B[38;5;66;03m# 4. optimized permutation batching\u001B[39;00m\n\u001B[0;32m    273\u001B[0m \u001B[38;5;66;03m# 5. disabled static optimizations\u001B[39;00m\n\u001B[1;32m--> 275\u001B[0m indices_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrange\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    276\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m shuffle \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    277\u001B[0m   indices_dataset \u001B[38;5;241m=\u001B[39m indices_dataset\u001B[38;5;241m.\u001B[39mrepeat(epochs)\n",
      "File \u001B[1;32mc:\\users\\suprabhashsahu\\desktop\\strategyresearch\\venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:1180\u001B[0m, in \u001B[0;36mDatasetV2.range\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   1144\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m   1145\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrange\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m   1146\u001B[0m   \u001B[38;5;124;03m\"\"\"Creates a `Dataset` of a step-separated range of values.\u001B[39;00m\n\u001B[0;32m   1147\u001B[0m \n\u001B[0;32m   1148\u001B[0m \u001B[38;5;124;03m  >>> list(Dataset.range(5).as_numpy_iterator())\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1178\u001B[0m \u001B[38;5;124;03m    ValueError: if len(args) == 0.\u001B[39;00m\n\u001B[0;32m   1179\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1180\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m RangeDataset(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\users\\suprabhashsahu\\desktop\\strategyresearch\\venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4707\u001B[0m, in \u001B[0;36mRangeDataset.__init__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   4705\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_parse_args(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   4706\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_structure \u001B[38;5;241m=\u001B[39m tensor_spec\u001B[38;5;241m.\u001B[39mTensorSpec([], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_type)\n\u001B[1;32m-> 4707\u001B[0m variant_tensor \u001B[38;5;241m=\u001B[39m gen_dataset_ops\u001B[38;5;241m.\u001B[39mrange_dataset(\n\u001B[0;32m   4708\u001B[0m     start\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_start,\n\u001B[0;32m   4709\u001B[0m     stop\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stop,\n\u001B[0;32m   4710\u001B[0m     step\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_step,\n\u001B[0;32m   4711\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_common_args)\n\u001B[0;32m   4712\u001B[0m \u001B[38;5;28msuper\u001B[39m(RangeDataset, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(variant_tensor)\n",
      "File \u001B[1;32mc:\\users\\suprabhashsahu\\desktop\\strategyresearch\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:6057\u001B[0m, in \u001B[0;36mrange_dataset\u001B[1;34m(start, stop, step, output_types, output_shapes, metadata, name)\u001B[0m\n\u001B[0;32m   6055\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tld\u001B[38;5;241m.\u001B[39mis_eager:\n\u001B[0;32m   6056\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 6057\u001B[0m     _result \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_FastPathExecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   6058\u001B[0m \u001B[43m      \u001B[49m\u001B[43m_ctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mRangeDataset\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moutput_types\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   6059\u001B[0m \u001B[43m      \u001B[49m\u001B[43moutput_types\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moutput_shapes\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_shapes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmetadata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   6060\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _result\n\u001B[0;32m   6061\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m _core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptLqbYzMDYMd"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MtgNyS5l0apF"
   },
   "source": [
    "action_to_backtest_action = {\n",
    "    0: 1,\n",
    "    1: 0,\n",
    "    2: -1\n",
    "}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQ6rwYG6vDaD"
   },
   "source": [
    "### Same dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YKU03wEYDZaK"
   },
   "source": [
    "actions = []\n",
    "for j in range(WINDOW_SHAPE, DATASET_LENGTH, REWARD_TIME): \n",
    "    state_j = environment.get_state(j, WINDOW_SHAPE)\n",
    "    q_value_j = agent.get_value_action_value(state_j)\n",
    "    actions.append(action_to_backtest_action[np.argmax(q_value_j)])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ydfzppX4DwT4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "outputId": "e99ff98a-8bd2-4305-fcd0-600bd9087518"
   },
   "source": [
    "plt.figure(figsize = (15, 5))\n",
    "plt.plot(environment.data)\n",
    "for e, a in enumerate(actions):\n",
    "    e += WINDOW_SHAPE\n",
    "    if a == 1:\n",
    "        plt.scatter(e, environment.data.iloc[e], color = 'green')\n",
    "    elif a == -1:\n",
    "        plt.scatter(e, environment.data.iloc[e], color = 'red')\n",
    "    else:\n",
    "        pass\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hDHWBMBWzX4G",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "outputId": "98580eb0-c7c3-4155-dabd-9b9de46d45cb"
   },
   "source": [
    "backtest = pd.DataFrame({\n",
    "    'price': environment.data.values.flatten(),\n",
    "    'signal': [0] * WINDOW_SHAPE + actions\n",
    "})\n",
    "backtest['price_diff'] = backtest['price'].diff().shift(-1)\n",
    "(backtest['price_diff'] * backtest['signal']).cumsum().plot()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGydYxUFvFhe"
   },
   "source": [
    "### Noisy dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LPw2wDT_hcCJ"
   },
   "source": [
    "environment2 = Environment(DATASET_LENGTH, True, True)\n",
    "actions = []\n",
    "for j in range(WINDOW_SHAPE, DATASET_LENGTH, REWARD_TIME): \n",
    "    state_j = environment2.get_state(j, WINDOW_SHAPE)\n",
    "    q_value_j = agent.get_value_action_value(state_j)\n",
    "    actions.append(action_to_backtest_action[np.argmax(q_value_j)])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "xtRiCRcwvL0j",
    "outputId": "b9396889-88e4-4e00-d055-0a9751e3f727"
   },
   "source": [
    "plt.figure(figsize = (15, 5))\n",
    "plt.plot(environment2.data)\n",
    "for e, a in enumerate(actions):\n",
    "    e += WINDOW_SHAPE\n",
    "    if a == 1:\n",
    "        plt.scatter(e, environment2.data.iloc[e], color = 'green')\n",
    "    elif a == -1:\n",
    "        plt.scatter(e, environment2.data.iloc[e], color = 'red')\n",
    "    else:\n",
    "        pass\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "1mmWwuMdvimy",
    "outputId": "61cc06b5-524f-43d5-d60b-cd094fd661db"
   },
   "source": [
    "backtest = pd.DataFrame({\n",
    "    'price': environment2.data.values.flatten(),\n",
    "    'signal': [0] * WINDOW_SHAPE + actions\n",
    "})\n",
    "backtest['price_diff'] = backtest['price'].diff().shift(-1)\n",
    "(backtest['price_diff'] * backtest['signal']).cumsum().plot()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLQV1ShDwjS2"
   },
   "source": [
    "### Other function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rL6-wAu0wk5J"
   },
   "source": [
    "data_new = pd.DataFrame(\n",
    "    np.cos(np.arange(DATASET_LENGTH)/5.0) + \n",
    "    np.cos(np.arange(DATASET_LENGTH)/10) + \n",
    "    np.cos(np.arange(DATASET_LENGTH)/20) + \n",
    "    np.cos(np.arange(DATASET_LENGTH)/30)\n",
    "    )"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Rz3I53Ek8Y9Q"
   },
   "source": [
    "environment3 = Environment(DATASET_LENGTH, True, True, data_new)\n",
    "actions = []\n",
    "for j in range(WINDOW_SHAPE, DATASET_LENGTH, REWARD_TIME): \n",
    "    state_j = environment3.get_state(j, WINDOW_SHAPE)\n",
    "    q_value_j = agent.get_value_action_value(state_j)\n",
    "    actions.append(action_to_backtest_action[np.argmax(q_value_j)])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "Ji3YzkKJ9CN6",
    "outputId": "c01d9a44-8268-4195-e49b-81ea34a0d54d"
   },
   "source": [
    "plt.figure(figsize = (15, 5))\n",
    "plt.plot(environment3.data)\n",
    "for e, a in enumerate(actions):\n",
    "    e += WINDOW_SHAPE\n",
    "    if a == 1:\n",
    "        plt.scatter(e, environment3.data.iloc[e], color = 'green')\n",
    "    elif a == -1:\n",
    "        plt.scatter(e, environment3.data.iloc[e], color = 'red')\n",
    "    else:\n",
    "        pass\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "pnoKA7iD9CjR",
    "outputId": "f0dc9ea8-6c65-447f-d4ca-b47c6261a6cd"
   },
   "source": [
    "backtest = pd.DataFrame({\n",
    "    'price': environment3.data.values.flatten(),\n",
    "    'signal': [0] * WINDOW_SHAPE + actions\n",
    "})\n",
    "backtest['price_diff'] = backtest['price'].diff().shift(-1)\n",
    "(backtest['price_diff'] * backtest['signal']).cumsum().plot()"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}