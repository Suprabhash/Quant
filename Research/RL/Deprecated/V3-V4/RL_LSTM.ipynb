{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "plt.rcParams['axes.grid'] = False\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "plt.rcParams['axes.grid'] = False\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "plt.rcParams['axes.grid'] = False\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import InputLayer\n",
    "from keras.layers import Dense, LSTM\n",
    "import neptune.new as neptune\n",
    "from datetime import datetime\n",
    "from keras_visualizer import visualizer\n",
    "\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Users/suprabhashsahu/Desktop/StrategyResearch/venv/Graphviz/bin/'\n",
    "\n",
    "from Utils.add_features import add_fisher\n",
    "from Data.data_retrieval import get_data\n",
    "from Utils.neptune_ai_api_key import API_KEY\n",
    "\n",
    "np.random.seed(12)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def get_stock_data(symbol):\n",
    "    if symbol == 'sinx':\n",
    "        df = get_data(\".NSEI\", 'D')\n",
    "        df.drop(columns=[\"Volume\"], inplace=True)\n",
    "        df[\"Close\"] = df[\"Open\"] = df[\"High\"] = df[\"Low\"] = np.sin(df.index / 10 ) +2\n",
    "    else:\n",
    "        df = get_data(symbol, 'D')\n",
    "    df.set_index(\"Datetime\", inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "def f_discretize(values, num_states=10):\n",
    "    states_value = dict()\n",
    "    step_size = 1./num_states\n",
    "    for i in range(num_states):\n",
    "        if i == num_states - 1:\n",
    "            states_value[i] = values.max()\n",
    "        else:\n",
    "            states_value[i] = values.quantile((i+1)*step_size)\n",
    "    return states_value\n",
    "\n",
    "def value_to_state(value, states_value):\n",
    "    if np.isnan(value):\n",
    "        return np.nan\n",
    "    else:\n",
    "        for state, v in states_value.items():\n",
    "            if value <= v:\n",
    "                return str(state)\n",
    "        return str(state)\n",
    "\n",
    "def add_features(df, features):\n",
    "    lookbacks = []\n",
    "    all_states = []\n",
    "    for feature, lookback, discretize in [(feature[\"feature\"], feature[\"lookback\"], feature[\"discretize\"]) for feature in features]:\n",
    "        lookbacks.append(lookback)\n",
    "        if feature == \"Close\":\n",
    "            if discretize>0:\n",
    "                states = f_discretize(df[\"Close\"].iloc[lookback:int(df.shape[0]*0.8)], discretize)\n",
    "                df[f\"{feature}_state\"] = df['Close'].apply(lambda x : value_to_state(x, states))\n",
    "        if feature.startswith(\"Fisher\"):\n",
    "            df[feature] = add_fisher([df, lookback])[[f\"Fisher{lookback}\"]]\n",
    "            if discretize>0:\n",
    "                states = f_discretize(df[feature].iloc[lookback:int(df.shape[0]*0.8)], discretize)\n",
    "                df[f\"{feature}_state\"] = df[feature].apply(lambda x : value_to_state(x, states))\n",
    "        if feature.startswith(\"Momentum\"):\n",
    "            df[feature] = df[\"Close\"].diff(lookback)\n",
    "            if discretize>0:\n",
    "                states = f_discretize(df[feature].iloc[lookback:int(df.shape[0]*0.8)], discretize)\n",
    "                df[f\"{feature}_state\"] = df[feature].apply(lambda x : value_to_state(x, states))\n",
    "        try:\n",
    "            all_states.append({'Feature': feature, 'states':states})\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    df = df.iloc[max(lookbacks):]\n",
    "    return df, all_states\n",
    "\n",
    "def plot_performance(prices, actions_history, equity_curve):\n",
    "    # fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 7))\n",
    "    fig, (ax1, ax3) = plt.subplots(2, 1, figsize=(15, 7))\n",
    "    ax1.plot(prices, label='Close')\n",
    "    ax1_copy = ax1.twinx()\n",
    "    ax1_copy.plot(actions_history, label='Actions')\n",
    "    # ax2.plot(actions_history, label='Actions')\n",
    "    # ax2_copy = ax2.twinx()\n",
    "    # for feature in [feature[\"feature\"] for feature in features]:\n",
    "    #     ax2_copy.plot(df[feature], label=feature, color='green', ls='dotted')\n",
    "    # ax2_copy.axhline(0.0, ls='--', color='grey')\n",
    "    ax3.plot(equity_curve, label='Net worth')\n",
    "    ax3.plot([price*10000 / prices[0] for price in prices], label='Benchmark')\n",
    "    ax1.legend()\n",
    "    # ax2.legend()\n",
    "    ax3.legend()\n",
    "    plt.show()\n",
    "    return fig"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_q_learning(train, state_lookback, model, alpha, epsilon, gamma, episodes, all_states, all_actions, metric=\"absolute\"):\n",
    "\n",
    "    train_df = train.copy()\n",
    "    train_data = train_df[[\"Close\", \"state\"]]\n",
    "    returns_vs_episodes = []\n",
    "\n",
    "    arr = np.empty(shape=(0,len(all_states)))\n",
    "    for i, val in enumerate(np.array(train_data)):\n",
    "        if i<state_lookback:\n",
    "            arr = np.vstack((arr, np.empty(shape=(1,len(all_states)))))\n",
    "            continue\n",
    "        current_adj_close, state = val\n",
    "        state = int(state)\n",
    "        arr = np.vstack((arr,np.identity(len(all_states))[state:state + 1]))\n",
    "\n",
    "    for ii in tqdm(range(episodes)):\n",
    "\n",
    "        #Backtester initialisation\n",
    "        balance = 10000\n",
    "        net_worth = balance\n",
    "        in_position = False\n",
    "        position_value = 0.0\n",
    "        price_bought = 0.0\n",
    "        bet_bought = 0.0\n",
    "        actions_history = []\n",
    "        equity_curve = []\n",
    "        rewards = []\n",
    "        states = []\n",
    "        next_states = []\n",
    "        prices = []\n",
    "        current_q_all_states = []\n",
    "        next_q_all_states = []\n",
    "\n",
    "        q = model.predict(arr)\n",
    "        actions = (-1*q).argsort()\n",
    "\n",
    "        for i, val in enumerate(np.array(train_data)):\n",
    "            if i<state_lookback:\n",
    "                continue\n",
    "            current_adj_close, state = val\n",
    "            prices.append(current_adj_close)\n",
    "            prev_adj_close, _ = np.array(train_data)[i - 1]\n",
    "            states.append(arr[i])  #Needs to be changed for historical states>1\n",
    "            current_q_all_states.append(q[i])\n",
    "\n",
    "            # decide action\n",
    "            if epsilon > 0.1:\n",
    "                epsilon = epsilon / 1.2\n",
    "\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                action_priority = np.arange(0,len(all_actions))\n",
    "                np.random.shuffle(action_priority)\n",
    "            else:\n",
    "                action_priority = actions[i]\n",
    "\n",
    "            action = action_priority[0]\n",
    "            actions_history.append(action)\n",
    "\n",
    "            if not in_position:\n",
    "                if action == 1:  # OPEN LONG\n",
    "                    in_position = True\n",
    "                    price_bought = current_adj_close\n",
    "                    bet_bought = balance\n",
    "                    balance -= bet_bought\n",
    "                    position_value = bet_bought\n",
    "                    rewards.append(0)\n",
    "                else:  # KEEP LOOKING\n",
    "                    rewards.append(0)\n",
    "            else:\n",
    "                market_return = ((current_adj_close - price_bought) / price_bought)\n",
    "                if action == 1:  # HOLD LONG\n",
    "                    position_value = bet_bought * (1.0 + market_return)\n",
    "                    if metric==\"absolute\":\n",
    "                        rewards.append(bet_bought*market_return)\n",
    "                    else:\n",
    "                        rewards.append(market_return)\n",
    "                else:  # CLOSE LONG\n",
    "                    balance += bet_bought * (1.0 + market_return)\n",
    "                    in_position = False\n",
    "                    price_bought = 0.0\n",
    "                    bet_bought = 0.0\n",
    "                    position_value = 0.0\n",
    "                    rewards.append(0)\n",
    "\n",
    "            net_worth = balance + position_value\n",
    "            equity_curve.append(net_worth)\n",
    "\n",
    "            try:\n",
    "                next_states.append(int(np.array(train_data)[i + 1][1]))\n",
    "                next_q_all_states.append(q[i+1])\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        arr_fit_X = np.empty(shape=(0,len(all_states)))\n",
    "        arr_fit_Y = np.empty(shape=(0,len(all_actions)))\n",
    "        for state, action, reward, next_state, cq, nq in zip(states, actions_history, rewards, next_states, current_q_all_states, next_q_all_states):\n",
    "            target = ((1. - alpha) * cq[action]) + alpha * (reward + gamma * np.max(nq))\n",
    "            cq[action] = target\n",
    "            arr_fit_X = np.vstack((arr_fit_X,state))\n",
    "            arr_fit_Y = np.vstack((arr_fit_Y,cq.reshape(-1, len(all_actions))))\n",
    "\n",
    "        model.fit(arr_fit_X,arr_fit_Y,epochs=30, verbose=0)\n",
    "        episode_return = equity_curve[-1]/equity_curve[0]-1\n",
    "        print(f\"Episode Number: {ii+1}, Total return of episode: {episode_return}\")\n",
    "        # plot_performance(prices, actions_history, equity_curve)\n",
    "        returns_vs_episodes.append(episode_return)\n",
    "\n",
    "    return model, returns_vs_episodes\n",
    "\n",
    "def eval_q_learning(test_data, model, all_states, state_lookback, metric):\n",
    "\n",
    "    test_data = test_data[[\"Close\", \"state\"]]\n",
    "\n",
    "    arr = np.empty(shape=(0,len(all_states)))\n",
    "    for i, val in enumerate(np.array(test_data)):\n",
    "        if i<state_lookback:\n",
    "            arr = np.vstack((arr, np.empty(shape=(1,len(all_states)))))\n",
    "            continue\n",
    "        current_adj_close, state = val\n",
    "        state = int(state)\n",
    "        arr = np.vstack((arr,np.identity(len(all_states))[state:state + 1]))\n",
    "\n",
    "    #Backtester initialisation\n",
    "    balance = 10000\n",
    "    in_position = False\n",
    "    position_value = 0.0\n",
    "    price_bought = 0.0\n",
    "    bet_bought = 0.0\n",
    "    actions_history = []\n",
    "    equity_curve = []\n",
    "    rewards = []\n",
    "    states = []\n",
    "    prices = []\n",
    "    current_q_all_states = []\n",
    "\n",
    "    q = model.predict(arr)\n",
    "    actions = (-1*q).argsort()\n",
    "\n",
    "    for i, val in enumerate(np.array(test_data)):\n",
    "        if i<state_lookback:\n",
    "            continue\n",
    "        current_adj_close, state = val\n",
    "        prices.append(current_adj_close)\n",
    "        prev_adj_close, _ = np.array(test_data)[i - 1]\n",
    "        states.append(arr[i])\n",
    "        current_q_all_states.append(q[i])\n",
    "        action_priority = actions[i]\n",
    "\n",
    "        action = action_priority[0]\n",
    "        actions_history.append(action)\n",
    "\n",
    "        if not in_position:\n",
    "            if action == 1:  # OPEN LONG\n",
    "                in_position = True\n",
    "                price_bought = current_adj_close\n",
    "                bet_bought = balance\n",
    "                balance -= bet_bought\n",
    "                position_value = bet_bought\n",
    "                rewards.append(0)\n",
    "            else:  # KEEP LOOKING\n",
    "                rewards.append(0)\n",
    "        else:\n",
    "            market_return = ((current_adj_close - price_bought) / price_bought)\n",
    "            if action == 1:  # HOLD LONG\n",
    "                position_value = bet_bought * (1.0 + market_return)\n",
    "                if metric==\"absolute\":\n",
    "                    rewards.append(bet_bought*market_return)\n",
    "                else:\n",
    "                    rewards.append(market_return)\n",
    "            else:  # CLOSE LONG\n",
    "                balance += bet_bought * (1.0 + market_return)\n",
    "                in_position = False\n",
    "                price_bought = 0.0\n",
    "                bet_bought = 0.0\n",
    "                position_value = 0.0\n",
    "                rewards.append(0)\n",
    "\n",
    "        net_worth = balance + position_value\n",
    "        equity_curve.append(net_worth)\n",
    "    portfolio_return = equity_curve[-1]/equity_curve[0]-1\n",
    "    return plot_performance(prices, actions_history, equity_curve), portfolio_return"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "\n",
    "RECORD_EXPERIMENT = True\n",
    "save = {}\n",
    "save_images = {}\n",
    "save[\"ExperimentName\"] = f\"Run {datetime.now().strftime('%H:%M:%S')}: Experiments with Price and Momentum: .NSEI\"\n",
    "\n",
    "#Get data\n",
    "df = get_stock_data('.NSEI')\n",
    "train_len = int(df.shape[0]*0.8)\n",
    "\n",
    "features = [\n",
    "        # {\"feature\": \"Close\", \"lookback\": 0, \"discretize\": 20},\n",
    "        # {\"feature\": \"Momentum\", \"lookback\": 1, \"discretize\": 100}\n",
    "    {\"feature\": \"Fisher100\", \"lookback\": 150, \"discretize\": 20},\n",
    "    # {\"feature\": \"Fisher200\", \"lookback\": 200, \"discretize\": 10},\n",
    "    {\"feature\": \"Fisher300\", \"lookback\": 300, \"discretize\": 20},\n",
    "    ]\n",
    "save[\"features\"] = features\n",
    "df, all_states = add_features(df, features)\n",
    "\n",
    "df[\"state\"] = df[[f\"{feature['feature']}_state\" for feature in features]].agg(''.join, axis=1)\n",
    "states = [list(state['states']) for state in all_states]\n",
    "all_states = list(itertools.product(*states))\n",
    "all_states = [''.join(tuple([str(state) for state in all_state])) for all_state in all_states]\n",
    "\n",
    "all_states_dict = {}\n",
    "for i, state in enumerate(all_states):\n",
    "    all_states_dict[state] = i\n",
    "\n",
    "df.replace({\"state\": all_states_dict}, inplace=True)\n",
    "\n",
    "train_df = df.iloc[:train_len, :]\n",
    "test_df = df.iloc[train_len:, :]\n",
    "\n",
    "all_actions = {0: 'neutral', 1: 'long'}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "def get_model(num_states, num_actions, state_lookback, num_dense_layers, neurons_per_layer):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons_per_layer, return_sequences=True, input_shape=(state_lookback, num_states)))\n",
    "    for i in range(num_dense_layers):\n",
    "        if i==num_dense_layers-1:\n",
    "            model.add(LSTM(neurons_per_layer))\n",
    "        else:\n",
    "            model.add(LSTM(neurons_per_layer, return_sequences=True))\n",
    "    model.add(Dense(num_actions, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "state_lookback = 10\n",
    "model = get_model(len(all_states), len(all_actions), state_lookback,2, len(features)*2)\n",
    "# visualizer(model, format='png', view=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "train_data = train_df.copy()\n",
    "train_data = train_data[[\"Close\", \"state\"]]\n",
    "returns_vs_episodes = []\n",
    "\n",
    "arr = np.empty(shape=(0,len(all_states)))\n",
    "X_trai = []\n",
    "for i, val in enumerate(np.array(train_data)):\n",
    "    if i<state_lookback-1:\n",
    "        arr = np.vstack((arr, np.empty(shape=(1,len(all_states)))))\n",
    "        continue\n",
    "    current_adj_close, state = val\n",
    "    state = int(state)\n",
    "    arr = np.vstack((arr,np.identity(len(all_states))[state:state + 1]))\n",
    "    X_trai.append(arr[i-state_lookback+1:i+1])\n",
    "X_trai = np.array(X_trai)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "data": {
      "text/plain": "(2887, 10, 400)"
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trai.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "q = model.predict(X_trai)\n",
    "actions = (-1*q).argsort()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/91 [==============================] - 8s 11ms/step - loss: nan - mae: nan    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1c287706b20>"
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_trai, actions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "data": {
      "text/plain": "(2887, 2)"
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 1],\n       [0, 1],\n       [0, 1],\n       ...,\n       [1, 0],\n       [1, 0],\n       [1, 0]], dtype=int64)"
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "alpha = save[\"alpha\"] = 0.1\n",
    "epsilon = save[\"epsilon\"] = 0.1\n",
    "gamma = save[\"gamma\"] = 0.1\n",
    "episodes = save[\"episodes\"] = 100\n",
    "metric = save[\"metric\"] = \"absolute\"\n",
    "\n",
    "model, returns_vs_episodes = train_q_learning(train_df, state_lookback, model, alpha, epsilon, gamma, episodes, all_states, all_actions, metric)\n",
    "fig, score = eval_q_learning(test_df, model, all_states, state_lookback, metric)\n",
    "save_images[\"TestResults\"] = fig\n",
    "if not (RECORD_EXPERIMENT):\n",
    "    plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(returns_vs_episodes)\n",
    "save_images[\"TrainResultsvsEpisodes\"] = fig\n",
    "if not (RECORD_EXPERIMENT):\n",
    "    plt.show()\n",
    "\n",
    "if RECORD_EXPERIMENT:\n",
    "    run = neptune.init(project=\"suprabhash/RL-MLP\", api_token=API_KEY)\n",
    "    for key in save_images.keys():\n",
    "        run[key].upload(save_images[key])\n",
    "    for key in save.keys():\n",
    "        run[key] = save[key]\n",
    "else:\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "alpha = save[\"alpha\"] = 0.1\n",
    "epsilon = save[\"epsilon\"] = 0.1\n",
    "gamma = save[\"gamma\"] = 0.1\n",
    "episodes = save[\"episodes\"] = 100\n",
    "metric = save[\"metric\"] = \"absolute\"\n",
    "\n",
    "model, returns_vs_episodes = train_q_learning(train_df, state_lookback, model, alpha, epsilon, gamma, episodes, all_states, all_actions, metric)\n",
    "fig, score = eval_q_learning(test_df, model, all_states, state_lookback, metric)\n",
    "save_images[\"TestResults\"] = fig\n",
    "if not (RECORD_EXPERIMENT):\n",
    "    plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(returns_vs_episodes)\n",
    "save_images[\"TrainResultsvsEpisodes\"] = fig\n",
    "if not (RECORD_EXPERIMENT):\n",
    "    plt.show()\n",
    "\n",
    "if RECORD_EXPERIMENT:\n",
    "    run = neptune.init(project=\"suprabhash/RL-MLP\", api_token=API_KEY)\n",
    "    for key in save_images.keys():\n",
    "        run[key].upload(save_images[key])\n",
    "    for key in save.keys():\n",
    "        run[key] = save[key]\n",
    "else:\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "alpha = save[\"alpha\"] = 0.1\n",
    "epsilon = save[\"epsilon\"] = 0.1\n",
    "gamma = save[\"gamma\"] = 0.1\n",
    "episodes = save[\"episodes\"] = 100\n",
    "metric = save[\"metric\"] = \"absolute\"\n",
    "\n",
    "model, returns_vs_episodes = train_q_learning(train_df, state_lookback, model, alpha, epsilon, gamma, episodes, all_states, all_actions, metric)\n",
    "fig, score = eval_q_learning(test_df, model, all_states, state_lookback, metric)\n",
    "save_images[\"TestResults\"] = fig\n",
    "if not (RECORD_EXPERIMENT):\n",
    "    plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(returns_vs_episodes)\n",
    "save_images[\"TrainResultsvsEpisodes\"] = fig\n",
    "if not (RECORD_EXPERIMENT):\n",
    "    plt.show()\n",
    "\n",
    "if RECORD_EXPERIMENT:\n",
    "    run = neptune.init(project=\"suprabhash/RL-MLP\", api_token=API_KEY)\n",
    "    for key in save_images.keys():\n",
    "        run[key].upload(save_images[key])\n",
    "    for key in save.keys():\n",
    "        run[key] = save[key]\n",
    "else:\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "alpha = save[\"alpha\"] = 0.1\n",
    "epsilon = save[\"epsilon\"] = 0.1\n",
    "gamma = save[\"gamma\"] = 0.1\n",
    "episodes = save[\"episodes\"] = 100\n",
    "metric = save[\"metric\"] = \"absolute\"\n",
    "\n",
    "model, returns_vs_episodes = train_q_learning(train_df, state_lookback, model, alpha, epsilon, gamma, episodes, all_states, all_actions, metric)\n",
    "fig, score = eval_q_learning(test_df, model, all_states, state_lookback, metric)\n",
    "save_images[\"TestResults\"] = fig\n",
    "if not (RECORD_EXPERIMENT):\n",
    "    plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(returns_vs_episodes)\n",
    "save_images[\"TrainResultsvsEpisodes\"] = fig\n",
    "if not (RECORD_EXPERIMENT):\n",
    "    plt.show()\n",
    "\n",
    "if RECORD_EXPERIMENT:\n",
    "    run = neptune.init(project=\"suprabhash/RL-MLP\", api_token=API_KEY)\n",
    "    for key in save_images.keys():\n",
    "        run[key].upload(save_images[key])\n",
    "    for key in save.keys():\n",
    "        run[key] = save[key]\n",
    "else:\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "alpha = save[\"alpha\"] = 0.1\n",
    "epsilon = save[\"epsilon\"] = 0.1\n",
    "gamma = save[\"gamma\"] = 0.1\n",
    "episodes = save[\"episodes\"] = 100\n",
    "metric = save[\"metric\"] = \"absolute\"\n",
    "\n",
    "model, returns_vs_episodes = train_q_learning(train_df, state_lookback, model, alpha, epsilon, gamma, episodes, all_states, all_actions, metric)\n",
    "fig, score = eval_q_learning(test_df, model, all_states, state_lookback, metric)\n",
    "save_images[\"TestResults\"] = fig\n",
    "if not (RECORD_EXPERIMENT):\n",
    "    plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(returns_vs_episodes)\n",
    "save_images[\"TrainResultsvsEpisodes\"] = fig\n",
    "if not (RECORD_EXPERIMENT):\n",
    "    plt.show()\n",
    "\n",
    "if RECORD_EXPERIMENT:\n",
    "    run = neptune.init(project=\"suprabhash/RL-MLP\", api_token=API_KEY)\n",
    "    for key in save_images.keys():\n",
    "        run[key].upload(save_images[key])\n",
    "    for key in save.keys():\n",
    "        run[key] = save[key]\n",
    "else:\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "alpha = save[\"alpha\"] = 0.1\n",
    "epsilon = save[\"epsilon\"] = 0.1\n",
    "gamma = save[\"gamma\"] = 0.1\n",
    "episodes = save[\"episodes\"] = 100\n",
    "metric = save[\"metric\"] = \"absolute\"\n",
    "\n",
    "model, returns_vs_episodes = train_q_learning(train_df, state_lookback, model, alpha, epsilon, gamma, episodes, all_states, all_actions, metric)\n",
    "fig, score = eval_q_learning(test_df, model, all_states, state_lookback, metric)\n",
    "save_images[\"TestResults\"] = fig\n",
    "if not (RECORD_EXPERIMENT):\n",
    "    plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(returns_vs_episodes)\n",
    "save_images[\"TrainResultsvsEpisodes\"] = fig\n",
    "if not (RECORD_EXPERIMENT):\n",
    "    plt.show()\n",
    "\n",
    "if RECORD_EXPERIMENT:\n",
    "    run = neptune.init(project=\"suprabhash/RL-MLP\", api_token=API_KEY)\n",
    "    for key in save_images.keys():\n",
    "        run[key].upload(save_images[key])\n",
    "    for key in save.keys():\n",
    "        run[key] = save[key]\n",
    "else:\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "state_lookback = 1\n",
    "alpha = save[\"alpha\"] = 0.1\n",
    "epsilon = save[\"epsilon\"] = 0.1\n",
    "gamma = save[\"gamma\"] = 0.1\n",
    "episodes = save[\"episodes\"] = 100\n",
    "metric = save[\"metric\"] = \"absolute\"\n",
    "\n",
    "model, returns_vs_episodes = train_q_learning(train_df, state_lookback, model, alpha, epsilon, gamma, episodes, all_states, all_actions, metric)\n",
    "fig, score = eval_q_learning(test_df, model, all_states, state_lookback, metric)\n",
    "save_images[\"TestResults\"] = fig\n",
    "if not (RECORD_EXPERIMENT):\n",
    "    plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(returns_vs_episodes)\n",
    "save_images[\"TrainResultsvsEpisodes\"] = fig\n",
    "if not (RECORD_EXPERIMENT):\n",
    "    plt.show()\n",
    "\n",
    "if RECORD_EXPERIMENT:\n",
    "    run = neptune.init(project=\"suprabhash/RL-MLP\", api_token=API_KEY)\n",
    "    for key in save_images.keys():\n",
    "        run[key].upload(save_images[key])\n",
    "    for key in save.keys():\n",
    "        run[key] = save[key]\n",
    "else:\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    \n",
    "    model = get_model(len(all_states), len(all_actions), 2, len(features)*2)\n",
    "    # visualizer(model, format='png', view=True)\n",
    "\n",
    "    state_lookback = 1\n",
    "    alpha = save[\"alpha\"] = 0.1\n",
    "    epsilon = save[\"epsilon\"] = 0.1\n",
    "    gamma = save[\"gamma\"] = 0.1\n",
    "    episodes = save[\"episodes\"] = 100\n",
    "    metric = save[\"metric\"] = \"absolute\"\n",
    "\n",
    "    model, returns_vs_episodes = train_q_learning(train_df, state_lookback, model, alpha, epsilon, gamma, episodes, all_states, all_actions, metric)\n",
    "    fig, score = eval_q_learning(test_df, model, all_states, state_lookback, metric)\n",
    "    save_images[\"TestResults\"] = fig\n",
    "    if not (RECORD_EXPERIMENT):\n",
    "        plt.show()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(returns_vs_episodes)\n",
    "    save_images[\"TrainResultsvsEpisodes\"] = fig\n",
    "    if not (RECORD_EXPERIMENT):\n",
    "        plt.show()\n",
    "\n",
    "    if RECORD_EXPERIMENT:\n",
    "        run = neptune.init(project=\"suprabhash/RL-MLP\", api_token=API_KEY)\n",
    "        for key in save_images.keys():\n",
    "            run[key].upload(save_images[key])\n",
    "        for key in save.keys():\n",
    "            run[key] = save[key]\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    \n",
    "    model = get_model(len(all_states), len(all_actions), 2, len(features)*2)\n",
    "    # visualizer(model, format='png', view=True)\n",
    "\n",
    "    state_lookback = 1\n",
    "    alpha = save[\"alpha\"] = 0.1\n",
    "    epsilon = save[\"epsilon\"] = 0.1\n",
    "    gamma = save[\"gamma\"] = 0.1\n",
    "    episodes = save[\"episodes\"] = 100\n",
    "    metric = save[\"metric\"] = \"absolute\"\n",
    "\n",
    "    model, returns_vs_episodes = train_q_learning(train_df, state_lookback, model, alpha, epsilon, gamma, episodes, all_states, all_actions, metric)\n",
    "    fig, score = eval_q_learning(test_df, model, all_states, state_lookback, metric)\n",
    "    save_images[\"TestResults\"] = fig\n",
    "    if not (RECORD_EXPERIMENT):\n",
    "        plt.show()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(returns_vs_episodes)\n",
    "    save_images[\"TrainResultsvsEpisodes\"] = fig\n",
    "    if not (RECORD_EXPERIMENT):\n",
    "        plt.show()\n",
    "\n",
    "    if RECORD_EXPERIMENT:\n",
    "        run = neptune.init(project=\"suprabhash/RL-MLP\", api_token=API_KEY)\n",
    "        for key in save_images.keys():\n",
    "            run[key].upload(save_images[key])\n",
    "        for key in save.keys():\n",
    "            run[key] = save[key]\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    RECORD_EXPERIMENT = True\n",
    "    save = {}\n",
    "    save_images = {}\n",
    "    save[\"ExperimentName\"] = f\"Run {datetime.now().strftime('%H:%M:%S')}: Experiments with Price and Momentum: .NSEI\"\n",
    "\n",
    "    #Get data\n",
    "    df = get_stock_data('.NSEI')\n",
    "    train_len = int(df.shape[0]*0.8)\n",
    "\n",
    "    features = [\n",
    "            # {\"feature\": \"Close\", \"lookback\": 0, \"discretize\": 20},\n",
    "            # {\"feature\": \"Momentum\", \"lookback\": 1, \"discretize\": 100}\n",
    "        {\"feature\": \"Fisher100\", \"lookback\": 150, \"discretize\": 20},\n",
    "        # {\"feature\": \"Fisher200\", \"lookback\": 200, \"discretize\": 10},\n",
    "        {\"feature\": \"Fisher300\", \"lookback\": 300, \"discretize\": 20},\n",
    "        ]\n",
    "    save[\"features\"] = features\n",
    "    df, all_states = add_features(df, features)\n",
    "\n",
    "    df[\"state\"] = df[[f\"{feature['feature']}_state\" for feature in features]].agg(''.join, axis=1)\n",
    "    states = [list(state['states']) for state in all_states]\n",
    "    all_states = list(itertools.product(*states))\n",
    "    all_states = [''.join(tuple([str(state) for state in all_state])) for all_state in all_states]\n",
    "\n",
    "    all_states_dict = {}\n",
    "    for i, state in enumerate(all_states):\n",
    "        all_states_dict[state] = i\n",
    "\n",
    "    df.replace({\"state\": all_states_dict}, inplace=True)\n",
    "\n",
    "    train_df = df.iloc[:train_len, :]\n",
    "    test_df = df.iloc[train_len:, :]\n",
    "\n",
    "    all_actions = {0: 'neutral', 1: 'long'}\n",
    "    model = get_model(len(all_states), len(all_actions), 2, len(features)*2)\n",
    "    # visualizer(model, format='png', view=True)\n",
    "\n",
    "    state_lookback = 1\n",
    "    alpha = save[\"alpha\"] = 0.1\n",
    "    epsilon = save[\"epsilon\"] = 0.1\n",
    "    gamma = save[\"gamma\"] = 0.1\n",
    "    episodes = save[\"episodes\"] = 100\n",
    "    metric = save[\"metric\"] = \"absolute\"\n",
    "\n",
    "    model, returns_vs_episodes = train_q_learning(train_df, state_lookback, model, alpha, epsilon, gamma, episodes, all_states, all_actions, metric)\n",
    "    fig, score = eval_q_learning(test_df, model, all_states, state_lookback, metric)\n",
    "    save_images[\"TestResults\"] = fig\n",
    "    if not (RECORD_EXPERIMENT):\n",
    "        plt.show()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(returns_vs_episodes)\n",
    "    save_images[\"TrainResultsvsEpisodes\"] = fig\n",
    "    if not (RECORD_EXPERIMENT):\n",
    "        plt.show()\n",
    "\n",
    "    if RECORD_EXPERIMENT:\n",
    "        run = neptune.init(project=\"suprabhash/RL-MLP\", api_token=API_KEY)\n",
    "        for key in save_images.keys():\n",
    "            run[key].upload(save_images[key])\n",
    "        for key in save.keys():\n",
    "            run[key] = save[key]\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_stock_data(symbol):\n",
    "    if symbol == 'sinx':\n",
    "        df = get_data(\".NSEI\", 'D')\n",
    "        df.drop(columns=[\"Volume\"], inplace=True)\n",
    "        df[\"Close\"] = df[\"Open\"] = df[\"High\"] = df[\"Low\"] = np.sin(df.index / 10 ) +2\n",
    "    else:\n",
    "        df = get_data(symbol, 'D')\n",
    "    df.set_index(\"Datetime\", inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "def f_discretize(values, num_states=10):\n",
    "    states_value = dict()\n",
    "    step_size = 1./num_states\n",
    "    for i in range(num_states):\n",
    "        if i == num_states - 1:\n",
    "            states_value[i] = values.max()\n",
    "        else:\n",
    "            states_value[i] = values.quantile((i+1)*step_size)\n",
    "    return states_value\n",
    "\n",
    "def value_to_state(value, states_value):\n",
    "    if np.isnan(value):\n",
    "        return np.nan\n",
    "    else:\n",
    "        for state, v in states_value.items():\n",
    "            if value <= v:\n",
    "                return str(state)\n",
    "        return str(state)\n",
    "\n",
    "def add_features(df, features):\n",
    "    lookbacks = []\n",
    "    all_states = []\n",
    "    for feature, lookback, discretize in [(feature[\"feature\"], feature[\"lookback\"], feature[\"discretize\"]) for feature in features]:\n",
    "        lookbacks.append(lookback)\n",
    "        if feature == \"Close\":\n",
    "            if discretize>0:\n",
    "                states = f_discretize(df[\"Close\"].iloc[lookback:int(df.shape[0]*0.8)], discretize)\n",
    "                df[f\"{feature}_state\"] = df['Close'].apply(lambda x : value_to_state(x, states))\n",
    "        if feature.startswith(\"Fisher\"):\n",
    "            df[feature] = add_fisher([df, lookback])[[f\"Fisher{lookback}\"]]\n",
    "            if discretize>0:\n",
    "                states = f_discretize(df[feature].iloc[lookback:int(df.shape[0]*0.8)], discretize)\n",
    "                df[f\"{feature}_state\"] = df[feature].apply(lambda x : value_to_state(x, states))\n",
    "        if feature.startswith(\"Momentum\"):\n",
    "            df[feature] = df[\"Close\"].diff(lookback)\n",
    "            if discretize>0:\n",
    "                states = f_discretize(df[feature].iloc[lookback:int(df.shape[0]*0.8)], discretize)\n",
    "                df[f\"{feature}_state\"] = df[feature].apply(lambda x : value_to_state(x, states))\n",
    "        try:\n",
    "            all_states.append({'Feature': feature, 'states':states})\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    df = df.iloc[max(lookbacks):]\n",
    "    return df, all_states\n",
    "\n",
    "def get_model(num_states, num_actions, num_dense_layers, neurons_per_layer):\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(batch_input_shape=(1, num_states)))\n",
    "    for i in range(num_dense_layers):\n",
    "        model.add(Dense(neurons_per_layer, activation='relu'))\n",
    "    model.add(Dense(num_actions, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def plot_performance(prices, actions_history, equity_curve):\n",
    "    # fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 7))\n",
    "    fig, (ax1, ax3) = plt.subplots(2, 1, figsize=(15, 7))\n",
    "    ax1.plot(prices, label='Close')\n",
    "    ax1_copy = ax1.twinx()\n",
    "    ax1_copy.plot(actions_history, label='Actions')\n",
    "    # ax2.plot(actions_history, label='Actions')\n",
    "    # ax2_copy = ax2.twinx()\n",
    "    # for feature in [feature[\"feature\"] for feature in features]:\n",
    "    #     ax2_copy.plot(df[feature], label=feature, color='green', ls='dotted')\n",
    "    # ax2_copy.axhline(0.0, ls='--', color='grey')\n",
    "    ax3.plot(equity_curve, label='Net worth')\n",
    "    ax3.plot([price*10000 / prices[0] for price in prices], label='Benchmark')\n",
    "    ax1.legend()\n",
    "    # ax2.legend()\n",
    "    ax3.legend()\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "def train_q_learning(train, state_lookback, model, alpha, epsilon, gamma, episodes, all_states, all_actions, metric=\"absolute\"):\n",
    "\n",
    "    train_df = train.copy()\n",
    "    train_data = train_df[[\"Close\", \"state\"]]\n",
    "    returns_vs_episodes = []\n",
    "\n",
    "    arr = np.empty(shape=(0,len(all_states)))\n",
    "    for i, val in enumerate(np.array(train_data)):\n",
    "        if i<state_lookback:\n",
    "            arr = np.vstack((arr, np.empty(shape=(1,len(all_states)))))\n",
    "            continue\n",
    "        current_adj_close, state = val\n",
    "        state = int(state)\n",
    "        arr = np.vstack((arr,np.identity(len(all_states))[state:state + 1]))\n",
    "\n",
    "    for ii in tqdm(range(episodes)):\n",
    "\n",
    "        #Backtester initialisation\n",
    "        balance = 10000\n",
    "        net_worth = balance\n",
    "        in_position = False\n",
    "        position_value = 0.0\n",
    "        price_bought = 0.0\n",
    "        bet_bought = 0.0\n",
    "        actions_history = []\n",
    "        equity_curve = []\n",
    "        rewards = []\n",
    "        states = []\n",
    "        next_states = []\n",
    "        prices = []\n",
    "        current_q_all_states = []\n",
    "        next_q_all_states = []\n",
    "\n",
    "        q = model.predict(arr)\n",
    "        actions = (-1*q).argsort()\n",
    "\n",
    "        for i, val in enumerate(np.array(train_data)):\n",
    "            if i<state_lookback:\n",
    "                continue\n",
    "            current_adj_close, state = val\n",
    "            prices.append(current_adj_close)\n",
    "            prev_adj_close, _ = np.array(train_data)[i - 1]\n",
    "            states.append(arr[i])  #Needs to be changed for historical states>1\n",
    "            current_q_all_states.append(q[i])\n",
    "\n",
    "            # decide action\n",
    "            if epsilon > 0.1:\n",
    "                epsilon = epsilon / 1.2\n",
    "\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                action_priority = np.arange(0,len(all_actions))\n",
    "                np.random.shuffle(action_priority)\n",
    "            else:\n",
    "                action_priority = actions[i]\n",
    "\n",
    "            action = action_priority[0]\n",
    "            actions_history.append(action)\n",
    "\n",
    "            if not in_position:\n",
    "                if action == 1:  # OPEN LONG\n",
    "                    in_position = True\n",
    "                    price_bought = current_adj_close\n",
    "                    bet_bought = balance\n",
    "                    balance -= bet_bought\n",
    "                    position_value = bet_bought\n",
    "                    rewards.append(0)\n",
    "                else:  # KEEP LOOKING\n",
    "                    rewards.append(0)\n",
    "            else:\n",
    "                market_return = ((current_adj_close - price_bought) / price_bought)\n",
    "                if action == 1:  # HOLD LONG\n",
    "                    position_value = bet_bought * (1.0 + market_return)\n",
    "                    if metric==\"absolute\":\n",
    "                        rewards.append(bet_bought*market_return)\n",
    "                    else:\n",
    "                        rewards.append(market_return)\n",
    "                else:  # CLOSE LONG\n",
    "                    balance += bet_bought * (1.0 + market_return)\n",
    "                    in_position = False\n",
    "                    price_bought = 0.0\n",
    "                    bet_bought = 0.0\n",
    "                    position_value = 0.0\n",
    "                    rewards.append(0)\n",
    "\n",
    "            net_worth = balance + position_value\n",
    "            equity_curve.append(net_worth)\n",
    "\n",
    "            try:\n",
    "                next_states.append(int(np.array(train_data)[i + 1][1]))\n",
    "                next_q_all_states.append(q[i+1])\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        arr_fit_X = np.empty(shape=(0,len(all_states)))\n",
    "        arr_fit_Y = np.empty(shape=(0,len(all_actions)))\n",
    "        for state, action, reward, next_state, cq, nq in zip(states, actions_history, rewards, next_states, current_q_all_states, next_q_all_states):\n",
    "            target = ((1. - alpha) * cq[action]) + alpha * (reward + gamma * np.max(nq))\n",
    "            cq[action] = target\n",
    "            arr_fit_X = np.vstack((arr_fit_X,state))\n",
    "            arr_fit_Y = np.vstack((arr_fit_Y,cq.reshape(-1, len(all_actions))))\n",
    "\n",
    "        model.fit(arr_fit_X,arr_fit_Y,epochs=30, verbose=0)\n",
    "        episode_return = equity_curve[-1]/equity_curve[0]-1\n",
    "        print(f\"Episode Number: {ii+1}, Total return of episode: {episode_return}\")\n",
    "        # plot_performance(prices, actions_history, equity_curve)\n",
    "        returns_vs_episodes.append(episode_return)\n",
    "\n",
    "    return model, returns_vs_episodes\n",
    "\n",
    "def eval_q_learning(test_data, model, all_states, state_lookback, metric):\n",
    "\n",
    "    test_data = test_data[[\"Close\", \"state\"]]\n",
    "\n",
    "    arr = np.empty(shape=(0,len(all_states)))\n",
    "    for i, val in enumerate(np.array(test_data)):\n",
    "        if i<state_lookback:\n",
    "            arr = np.vstack((arr, np.empty(shape=(1,len(all_states)))))\n",
    "            continue\n",
    "        current_adj_close, state = val\n",
    "        state = int(state)\n",
    "        arr = np.vstack((arr,np.identity(len(all_states))[state:state + 1]))\n",
    "\n",
    "    #Backtester initialisation\n",
    "    balance = 10000\n",
    "    in_position = False\n",
    "    position_value = 0.0\n",
    "    price_bought = 0.0\n",
    "    bet_bought = 0.0\n",
    "    actions_history = []\n",
    "    equity_curve = []\n",
    "    rewards = []\n",
    "    states = []\n",
    "    prices = []\n",
    "    current_q_all_states = []\n",
    "\n",
    "    q = model.predict(arr)\n",
    "    actions = (-1*q).argsort()\n",
    "\n",
    "    for i, val in enumerate(np.array(test_data)):\n",
    "        if i<state_lookback:\n",
    "            continue\n",
    "        current_adj_close, state = val\n",
    "        prices.append(current_adj_close)\n",
    "        prev_adj_close, _ = np.array(test_data)[i - 1]\n",
    "        states.append(arr[i])\n",
    "        current_q_all_states.append(q[i])\n",
    "        action_priority = actions[i]\n",
    "\n",
    "        action = action_priority[0]\n",
    "        actions_history.append(action)\n",
    "\n",
    "        if not in_position:\n",
    "            if action == 1:  # OPEN LONG\n",
    "                in_position = True\n",
    "                price_bought = current_adj_close\n",
    "                bet_bought = balance\n",
    "                balance -= bet_bought\n",
    "                position_value = bet_bought\n",
    "                rewards.append(0)\n",
    "            else:  # KEEP LOOKING\n",
    "                rewards.append(0)\n",
    "        else:\n",
    "            market_return = ((current_adj_close - price_bought) / price_bought)\n",
    "            if action == 1:  # HOLD LONG\n",
    "                position_value = bet_bought * (1.0 + market_return)\n",
    "                if metric==\"absolute\":\n",
    "                    rewards.append(bet_bought*market_return)\n",
    "                else:\n",
    "                    rewards.append(market_return)\n",
    "            else:  # CLOSE LONG\n",
    "                balance += bet_bought * (1.0 + market_return)\n",
    "                in_position = False\n",
    "                price_bought = 0.0\n",
    "                bet_bought = 0.0\n",
    "                position_value = 0.0\n",
    "                rewards.append(0)\n",
    "\n",
    "        net_worth = balance + position_value\n",
    "        equity_curve.append(net_worth)\n",
    "    portfolio_return = equity_curve[-1]/equity_curve[0]-1\n",
    "    return plot_performance(prices, actions_history, equity_curve), portfolio_return\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    RECORD_EXPERIMENT = True\n",
    "    save = {}\n",
    "    save_images = {}\n",
    "    save[\"ExperimentName\"] = f\"Run {datetime.now().strftime('%H:%M:%S')}: Experiments with Price and Momentum: .NSEI\"\n",
    "\n",
    "    #Get data\n",
    "    df = get_stock_data('.NSEI')\n",
    "    train_len = int(df.shape[0]*0.8)\n",
    "\n",
    "    features = [\n",
    "            # {\"feature\": \"Close\", \"lookback\": 0, \"discretize\": 20},\n",
    "            # {\"feature\": \"Momentum\", \"lookback\": 1, \"discretize\": 100}\n",
    "        {\"feature\": \"Fisher100\", \"lookback\": 150, \"discretize\": 20},\n",
    "        # {\"feature\": \"Fisher200\", \"lookback\": 200, \"discretize\": 10},\n",
    "        {\"feature\": \"Fisher300\", \"lookback\": 300, \"discretize\": 20},\n",
    "        ]\n",
    "    save[\"features\"] = features\n",
    "    df, all_states = add_features(df, features)\n",
    "\n",
    "    df[\"state\"] = df[[f\"{feature['feature']}_state\" for feature in features]].agg(''.join, axis=1)\n",
    "    states = [list(state['states']) for state in all_states]\n",
    "    all_states = list(itertools.product(*states))\n",
    "    all_states = [''.join(tuple([str(state) for state in all_state])) for all_state in all_states]\n",
    "\n",
    "    all_states_dict = {}\n",
    "    for i, state in enumerate(all_states):\n",
    "        all_states_dict[state] = i\n",
    "\n",
    "    df.replace({\"state\": all_states_dict}, inplace=True)\n",
    "\n",
    "    train_df = df.iloc[:train_len, :]\n",
    "    test_df = df.iloc[train_len:, :]\n",
    "\n",
    "    all_actions = {0: 'neutral', 1: 'long'}\n",
    "    model = get_model(len(all_states), len(all_actions), 2, len(features)*2)\n",
    "    # visualizer(model, format='png', view=True)\n",
    "\n",
    "    state_lookback = 1\n",
    "    alpha = save[\"alpha\"] = 0.1\n",
    "    epsilon = save[\"epsilon\"] = 0.1\n",
    "    gamma = save[\"gamma\"] = 0.1\n",
    "    episodes = save[\"episodes\"] = 100\n",
    "    metric = save[\"metric\"] = \"absolute\"\n",
    "\n",
    "    model, returns_vs_episodes = train_q_learning(train_df, state_lookback, model, alpha, epsilon, gamma, episodes, all_states, all_actions, metric)\n",
    "    fig, score = eval_q_learning(test_df, model, all_states, state_lookback, metric)\n",
    "    save_images[\"TestResults\"] = fig\n",
    "    if not (RECORD_EXPERIMENT):\n",
    "        plt.show()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(returns_vs_episodes)\n",
    "    save_images[\"TrainResultsvsEpisodes\"] = fig\n",
    "    if not (RECORD_EXPERIMENT):\n",
    "        plt.show()\n",
    "\n",
    "    if RECORD_EXPERIMENT:\n",
    "        run = neptune.init(project=\"suprabhash/RL-MLP\", api_token=API_KEY)\n",
    "        for key in save_images.keys():\n",
    "            run[key].upload(save_images[key])\n",
    "        for key in save.keys():\n",
    "            run[key] = save[key]\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "plt.rcParams['axes.grid'] = False\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}